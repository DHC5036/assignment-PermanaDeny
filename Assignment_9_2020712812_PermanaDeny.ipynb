{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "DHC_Assignment-9_2020712812.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MWwTaEkcsMsz"
      },
      "source": [
        "# Q1 "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sbF8RrSBsYEz"
      },
      "source": [
        "## Q\n",
        "What happens to the gradient if you backpropagate through a long sequence?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1z1g17RhsQ_D"
      },
      "source": [
        "## A\n",
        "Take the small-valued number into long mathematical multiplication, that it yields much more small value. \n",
        "\n",
        "It happends the same way in long sequence back propagation from the output go through the input. The result of the update value will shrink into very small number. In machine learning, it calls gradient vanishing."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ig_ldi-_tS4v"
      },
      "source": [
        "# Q2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cs8Q_bPGtUc3"
      },
      "source": [
        "## Q\n",
        "Compare the abstracted computational cost for GRUs, LSTMs, and regular RNNs for a given hidden dimension. Pay special attention to the training and inference cost."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5XC8hyhFtc_u"
      },
      "source": [
        "## A\n",
        "Given the same hidden dimension of GRU, LSTM, and RNN, yield different number of trainable parameters due to their architecture approach.\n",
        "Regardless of the model performance, the RNN has the smallest  trainable parameters, following GRU and LSTM.\n",
        "\n",
        "In the training process, it implies the computational cost of RNN is lower than GRU and LSTM respectively, to be exact due to the back propagation process. \n",
        "\n",
        "So do for the inference process. Due to the number of mathematical operations inside the architecture, the inference cost order becomes RNN, GRU, and LSTM the largest one.\n",
        "\n",
        "\n"
      ]
    }
  ]
}